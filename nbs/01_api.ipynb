{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# api\n",
    "\n",
    "This is the primary interface to running squ wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas, json, logging, time, requests, io, pkgutil\n",
    "from nbdev_squ.core import *\n",
    "from diskcache import memoize_stampede\n",
    "from importlib.metadata import version\n",
    "from subprocess import run, CalledProcessError\n",
    "from azure.monitor.query import LogsQueryClient, LogsBatchQuery, LogsQueryStatus\n",
    "from azure.identity import AzureCliCredential\n",
    "from benedict import benedict\n",
    "from functools import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Clients\n",
    "\n",
    "Returns preconfigured clients for various services, cached for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Clients:\n",
    "    \"\"\"\n",
    "    Clients for various services, cached for performance\n",
    "    \"\"\"\n",
    "    @cached_property\n",
    "    def config(self):\n",
    "        login()\n",
    "        return cache.get(\"config\", load_config())\n",
    "\n",
    "    @cached_property\n",
    "    def runzero(self):\n",
    "        \"\"\"\n",
    "        Returns a runzero client\n",
    "        \"\"\"\n",
    "        import httpx_cache\n",
    "        return httpx_cache.Client(base_url=\"https://console.rumble.run/api/v1.0\", headers={\"Authorization\": f\"Bearer {self.config.runzero_apitoken}\"})\n",
    "\n",
    "    @cached_property\n",
    "    def abuseipdb(self):\n",
    "        \"\"\"\n",
    "        Returns an abuseipdb client\n",
    "        \"\"\"\n",
    "        from abuseipdb_wrapper import AbuseIPDB\n",
    "        return AbuseIPDB(api_key=self.config.abuseipdb_api_key)\n",
    "\n",
    "    @cached_property\n",
    "    def jira(self):\n",
    "        \"\"\"\n",
    "        Returns a jira client\n",
    "        \"\"\"\n",
    "        from atlassian import Jira\n",
    "        return Jira(url=self.config.jira_url, username=self.config.jira_username, password=self.config.jira_password)\n",
    "\n",
    "    @cached_property\n",
    "    def tio(self):\n",
    "        \"\"\"\n",
    "        Returns a TenableIO client\n",
    "        \"\"\"\n",
    "        from tenable.io import TenableIO\n",
    "        return TenableIO(self.config.tenable_access_key, self.config.tenable_secret_key)\n",
    "\n",
    "\n",
    "clients = Clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunZero\n",
    "response = clients.runzero.get(\"/export/org/assets.csv\", params={\"search\": \"has_public:t AND alive:t AND (protocol:rdp OR protocol:vnc OR protocol:teamviewer OR protocol:telnet OR protocol:ftp)\"})\n",
    "pandas.read_csv(io.StringIO(response.text)).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jira\n",
    "pandas.json_normalize(clients.jira.jql(\"updated > -1d\")[\"issues\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AbuseIPDB\n",
    "clients.abuseipdb.check_ip(\"1.1.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TenableIO\n",
    "pandas.DataFrame(clients.tio.scans.list()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Workspaces\n",
    "\n",
    "The `list_workspaces` function retreives a list of workspaces from blob storage and returns it in various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_workspaces(fmt: str = \"df\", # df, csv, json, list\n",
    "                    agency: str = \"ALL\"): # Agency alias or ALL\n",
    "    path = datalake_path()\n",
    "    df = pandas.read_csv((path / \"notebooks/lists/SentinelWorkspaces.csv\").open())\n",
    "    df = df.join(pandas.read_csv((path / \"notebooks/lists/SecOps Groups.csv\").open()).set_index(\"Alias\"), on=\"SecOps Group\", rsuffix=\"_secops\")\n",
    "    df = df.rename(columns={\"SecOps Group\": \"alias\", \"Domains and IPs\": \"domains\"})\n",
    "    df = df.dropna(subset=[\"customerId\"]).sort_values(by=\"alias\").convert_dtypes().reset_index()\n",
    "    if agency != \"ALL\":\n",
    "        df = df[df[\"alias\"] == agency]\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    elif fmt == \"list\":\n",
    "        return list(df[\"customerId\"].unique())\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use it do lookup an agencies alias based on its `customerId` (also known as `TenantId`, Log Analytics `WorkspaceId`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces = list_workspaces()\n",
    "workspaces.query(f'customerId == \"{workspaces[\"customerId\"][0]}\"')[\"alias\"].str.cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analytics Query\n",
    "The below function makes it easy to query all workspaces with sentinel installed using log analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_subscriptions():\n",
    "    return pandas.DataFrame(azcli([\"account\", \"list\"]))[\"id\"].unique()\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_securityinsights():\n",
    "    return pandas.DataFrame(azcli([\n",
    "        \"graph\", \"query\", \"--first\", \"1000\", \"-q\", \n",
    "        \"\"\"\n",
    "        resources\n",
    "        | where type =~ 'microsoft.operationsmanagement/solutions'\n",
    "        | where name startswith 'SecurityInsights'\n",
    "        | project wlid = tolower(tostring(properties.workspaceResourceId))\n",
    "        | join kind=leftouter (\n",
    "            resources | where type =~ 'microsoft.operationalinsights/workspaces' | extend wlid = tolower(id))\n",
    "            on wlid\n",
    "        | extend customerId = properties.customerId\n",
    "        \"\"\"\n",
    "    ])[\"data\"])\n",
    "\n",
    "def chunks(items, size):\n",
    "    # Yield successive `size` chunks from `items`\n",
    "    for i in range(0, len(items), size):\n",
    "        yield items[i:i + size]\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 5) # cache for 5 mins\n",
    "def loganalytics_query(queries: list[str], timespan=pandas.Timedelta(\"14d\"), batch_size=190, batch_delay=32, sentinel_workspaces = None):\n",
    "    \"\"\"\n",
    "    Run queries across all workspaces, in batches of `batch_size` with a minimum delay of `batch_delay` between batches.\n",
    "    Returns a dictionary of queries and results\n",
    "    \"\"\"\n",
    "    client = LogsQueryClient(AzureCliCredential())\n",
    "    workspaces = list_workspaces(fmt=\"df\")\n",
    "    if sentinel_workspaces is None:\n",
    "        sentinel_workspaces = list_securityinsights()\n",
    "    requests, results = [], []\n",
    "    for query in queries:\n",
    "        for workspace_id in sentinel_workspaces[\"customerId\"]:\n",
    "            requests.append(LogsBatchQuery(workspace_id=workspace_id, query=query, timespan=timespan))\n",
    "    querytime = pandas.Timestamp(\"now\")\n",
    "    logger.info(f\"Executing {len(requests)} queries at {querytime}\")\n",
    "    for request_batch in chunks(requests, batch_size):\n",
    "        while cache.get(\"loganalytics_query_running\"):\n",
    "            time.sleep(1)\n",
    "        batch_start = pandas.Timestamp(\"now\")\n",
    "        cache.set(\"loganalytics_query_running\", True, batch_delay)\n",
    "        results += client.query_batch(request_batch)\n",
    "        duration = pandas.Timestamp(\"now\") - batch_start\n",
    "        logger.info(f\"Completed {len(results)} (+{len(request_batch)}) in {duration}\")\n",
    "    dfs = {}\n",
    "    for request, result in zip(requests, results):\n",
    "        if result.status == LogsQueryStatus.PARTIAL:\n",
    "            tables = result.partial_data\n",
    "            tables = [pandas.DataFrame(table.rows, columns=table.columns) for table in tables]\n",
    "        elif result.status == LogsQueryStatus.SUCCESS:\n",
    "            tables = result.tables\n",
    "            tables = [pandas.DataFrame(table.rows, columns=table.columns) for table in tables]\n",
    "        else:\n",
    "            tables = [pandas.DataFrame([result.__dict__])]\n",
    "        df = pandas.concat(tables).dropna(axis=1, how='all') # prune empty columns\n",
    "        df[\"TenantId\"] = request.workspace\n",
    "        alias = workspaces.query(f'customerId == \"{request.workspace}\"')[\"alias\"].str.cat()\n",
    "        if alias == '':\n",
    "            alias = sentinel_workspaces.query(f'customerId == \"{request.workspace}\"')[\"name\"].str.cat()\n",
    "        df[\"_alias\"] = alias\n",
    "        query = request.body[\"query\"]\n",
    "        if query in dfs:\n",
    "            dfs[query].append(df)\n",
    "        else:\n",
    "            dfs[query] = [df]\n",
    "    return {query: pandas.concat(results, ignore_index=True).convert_dtypes() for query, results in dfs.items()}\n",
    "\n",
    "def query_all(query, fmt=\"df\", timespan=pandas.Timedelta(\"14d\")):\n",
    "    try:\n",
    "        # Check query is not a plain string and is iterable\n",
    "        assert not isinstance(query, str)\n",
    "        iter(query)\n",
    "    except (AssertionError, TypeError):\n",
    "        # if it is a plain string or it's not iterable, convert into a list of queries\n",
    "        query = [query]\n",
    "    df = pandas.concat(loganalytics_query(query, timespan).values())\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all([\"SecurityAlert\"]).groupby(\"_alias\")[[\"Tactics\", \"TenantId\"]].nunique().sort_values(by=\"Tactics\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat hunting helper\n",
    "\n",
    "Scan across all databases and tables for columns with a given predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "columns_of_interest = benedict({\n",
    "    \"name\": ['AADEmail', 'AccountName', 'AccountUPN', 'AccountUpn', 'Account', 'CompromisedEntity', 'DestinationUserName', \n",
    "             \"Computer\", \"DisplayName\", \"EmailSenderAddress\", \"FileName\", 'FilePath', \"FolderPath\", 'FullyQualifiedSubjectUserName', 'InitiatingProcessAccountUpn',\n",
    "            'MailboxOwnerUPN', 'Name', 'NewProcessName', 'Owner', 'ParentProcessName', 'Process', 'CommandLine', 'ProcessCommandLine', 'RecipientEmailAddress',\n",
    "            'RequesterUpn', 'SenderMailFromAddress', 'SourceIdentity', 'SourceUserName', 'SubjectUserName', 'TargetUserName', 'TargetUser', 'Upn',\n",
    "            'UserName', 'userName', 'UserPrincipalName'],\n",
    "    \"guid\": ['Caller', \"DestinationUserID\", 'SourceUserID', 'UserId'],\n",
    "    \"ip\": ['CallerIpAddress', 'DestinationIP', 'DstIpAddr', 'EmailSourceIpAddress', 'IPAddresses', 'IPAddress', 'IpAddress',\n",
    "           'NetworkDestinationIP', 'NetworkIP', 'NetworkSourceIP', 'RemoteIP', 'SourceIP', 'SrcIpAddr'],\n",
    "    \"url\": [\"DomainName\", 'FileOriginUrl', 'FQDN', 'RemoteUrl', 'Url'],\n",
    "    \"hash\": [\"MD5\", \"SHA1\", \"SHA256\", 'FileHashValue', 'FileHash', 'InitiatingProcessSHA256']\n",
    "})\n",
    "\n",
    "columns = [column for area in columns_of_interest.values() for column in area]\n",
    "\n",
    "def finalise_query(query, take):\n",
    "    return f\"{query} | take {take} | extend placeholder_=dynamic({{'':null}}) | evaluate bag_unpack(column_ifexists('pack_', placeholder_))\"\n",
    "\n",
    "def hunt(indicators, expression=\"has\", columns=columns, workspaces=None, timespans=[\"1d\", \"14d\", \"90d\", \"700d\"], take=5000):\n",
    "    queries = []\n",
    "    if workspaces is None:\n",
    "        workspaces = list_securityinsights()\n",
    "    else:\n",
    "        df = list_securityinsights()\n",
    "        workspaces = df[df[\"customerId\"].isin(workspaces)]\n",
    "    querylogged = False\n",
    "    if expression in ['has_any']:\n",
    "        query = f\"let indicators = dynamic({indicators}); \"\n",
    "\n",
    "        for count, column in enumerate(columns):\n",
    "            if count == 0:\n",
    "                query += f\"find where {column} has_any (indicators)\"\n",
    "            else:\n",
    "                query += f\" or {column} has_any (indicators)\"\n",
    "        final_query = finalise_query(query, take)\n",
    "        queries.append(final_query)\n",
    "    else:\n",
    "        for indicator in indicators:\n",
    "            if expression not in ['has_all']:\n",
    "                indicator = f\"'{indicator}'\" # wrap indicator in quotes unless expecting dynamic\n",
    "            if not querylogged:\n",
    "                logger.info(f\"Test Query: find where {columns[0]} {expression} {indicator} | take {take}\")\n",
    "                querylogged = True\n",
    "            for chunk in chunks([f\"{column} {expression} {indicator}\" for column in columns], 20):\n",
    "                query = \" or \".join(chunk)\n",
    "                final_query = finalise_query(f\"find where {query}\", take)\n",
    "                queries.append(final_query)\n",
    "    for timespan in timespans:\n",
    "        results = pandas.concat(loganalytics_query(queries, pandas.Timedelta(timespan), sentinel_workspaces = workspaces).values())\n",
    "        if 'placeholder_' in results.columns:\n",
    "            results = results.drop('placeholder_', axis=1)\n",
    "        if results.empty:\n",
    "            logger.info(f\"No results in {timespan}, extending hunt\")\n",
    "            continue\n",
    "        logger.info(f\"Found {indicators} in {timespan}, returning\")\n",
    "        return results\n",
    "    else:\n",
    "        raise Exception(\"No results found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hunt(['(\"ntdsutil\", \"ifm\")'], expression=\"has_all\", columns=[\"InitiatingProcessCommandLine\", \"ProcessCommandLine\", \"CommandLine\"], timespans=[\"90d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def atlaskit_transformer(inputtext, inputfmt=\"md\", outputfmt=\"wiki\", runtime=\"node\"):\n",
    "    transformer = dirs.user_cache_path / f\"atlaskit-transformer.bundle_v{version('nbdev_squ')}.js\"\n",
    "    if not transformer.exists():\n",
    "        transformer.write_bytes(pkgutil.get_data(\"nbdev_squ\", \"atlaskit-transformer.bundle.js\"))\n",
    "    cmd = [runtime, str(transformer), inputfmt, outputfmt]\n",
    "    logger.debug(\" \".join(cmd))\n",
    "    try:\n",
    "        return run(cmd, input=inputtext, text=True, capture_output=True, check=True).stdout\n",
    "    except CalledProcessError:\n",
    "        run(cmd, input=inputtext, text=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(atlaskit_transformer(\"\"\"# Heading 1\n",
    "\n",
    "- a bullet\n",
    "[a link](https://github.com]\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel Incident sync helpers\n",
    "\n",
    "Overall process below will be to grab all the incidents from Sentinel, and save them into individual files per day based on their last updated time. A subsequent process can then load those files (e.g. ingest into ADX) and/or synchronise the updates into Jira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def security_incidents(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"1d\"), timedelta=pandas.Timedelta(\"1d\")):\n",
    "    # Queries for security incidents from `start` time for `timedelta` and returns a dataframe\n",
    "    # Sorts by TimeGenerated (TODO)\n",
    "    query = \"SecurityIncident | summarize arg_max(TimeGenerated, *) by IncidentNumber\"\n",
    "    return query_all(query, timespan=(start.to_pydatetime(), timedelta))\n",
    "\n",
    "def security_alerts(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"1d\"), timedelta=pandas.Timedelta(\"1d\")):\n",
    "    # Queries for security alerts from `start` time for `timedelta` and returns a dataframe\n",
    "    # Sorts by TimeGenerated (TODO)\n",
    "    query = \"SecurityAlert | summarize arg_max(TimeGenerated, *) by SystemAlertId\"\n",
    "    return query_all(query, timespan=(start.to_pydatetime(), timedelta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = security_incidents(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"14d\"), timedelta=pandas.Timedelta(\"14d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
