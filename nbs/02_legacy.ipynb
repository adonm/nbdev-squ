{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac93b06e-85ef-4520-991e-c8066e45533b",
   "metadata": {},
   "source": [
    "# legacy\n",
    "\n",
    "These are some legacy utilities and integrations which are no longer actively maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96712b92-6b40-4b40-b854-4711d29f7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25975a-a4bd-4d8b-bf0c-e4d4d08765ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d53d6d-7265-4ea3-a696-159c88e72b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from nbdev_squ import api\n",
    "from markdown import markdown\n",
    "from diskcache import memoize_stampede\n",
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "import logging, pandas, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef431c-1423-4095-8b6f-cddf8f5323eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0d5cf-eba8-42df-ba90-fd357505f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df032ed2-0117-4b44-8d39-058ea6989f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(api.cache, expire=60 * 5) # cache for 5 mins\n",
    "def adx_query(kql):\n",
    "    \"\"\"\n",
    "    Run a kusto query\n",
    "\n",
    "    Args:\n",
    "        kql (str or list): kusto query or list of queries\n",
    "\n",
    "    Returns:\n",
    "        json: query results\n",
    "    \"\"\"\n",
    "    if isinstance(kql, list):\n",
    "        kql = [\".execute script with (ContinueOnErrors=true) <|\"] + kql\n",
    "        kql = \"\\n\".join(kql)\n",
    "    config = api.cache[\"config\"]\n",
    "    cluster, dx_db = config.azure_dataexplorer.rsplit(\"/\", 1)\n",
    "    dx_client = KustoClient(KustoConnectionStringBuilder.with_az_cli_authentication(cluster))\n",
    "    return dx_client.execute(dx_db, kql.replace(\"\\\\\", \"\\\\\\\\\")).primary_results[0]\n",
    "\n",
    "def adxtable2df(table):\n",
    "    \"\"\"\n",
    "    Return a pandas dataframe from an adx table\n",
    "    \"\"\"\n",
    "    columns = [col.column_name for col in table.columns]\n",
    "    frame = pandas.DataFrame(table.raw_rows, columns=columns)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554837e-f47c-4e8c-9cb0-9aacc1e427b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adxtable2df(adx_query(\"SecurityAlert | take 10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df9e5a-c337-49f2-a959-f850649a6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def export_jira_issues():\n",
    "    \"\"\"\n",
    "    Exports all JIRA issues to the data lake.\n",
    "    \"\"\"\n",
    "    jira_issues_path = api.datalake_path() / \"jira_outputs\" / \"issues\"\n",
    "\n",
    "    def getissues(start_at, jql):\n",
    "        response = api.clients.jira.jql(jql, start=start_at, limit=100)\n",
    "        next_start = response[\"startAt\"] + response[\"maxResults\"]\n",
    "        total_rows = response[\"total\"]\n",
    "        if next_start > total_rows:\n",
    "            next_start = total_rows\n",
    "        issues = response[\"issues\"]\n",
    "        return next_start, total_rows, issues\n",
    "\n",
    "    def save_date_issues(after_date: pandas.Timestamp, path=jira_issues_path):\n",
    "        fromdate = after_date\n",
    "        todate = after_date + pandas.to_timedelta(\"1d\")\n",
    "        jql = f\"updated >= {fromdate.date().isoformat()} and updated < {todate.date().isoformat()} order by key\"\n",
    "        output = path / f\"{fromdate.date().isoformat()}\" / \"issues.parquet\"\n",
    "        if output.exists() and fromdate < pandas.Timestamp.now() - pandas.to_timedelta(\"1d\"):\n",
    "            # skip previously dumped days except for last day\n",
    "            return None\n",
    "        start_at, total_rows = 0, -1\n",
    "        dataframes = []\n",
    "        while start_at != total_rows:\n",
    "            start_at, total_rows, issues = getissues(start_at, jql)\n",
    "            dataframes.append(pandas.DataFrame(issues))\n",
    "            if start_at == 100:\n",
    "                logger.info(f\"{total_rows} to load\")\n",
    "        if total_rows > 1:\n",
    "            df = pandas.concat(dataframes)\n",
    "            df[\"fields\"] = df[\"fields\"].apply(json.dumps)\n",
    "            logger.info(f\"saving {output}\")\n",
    "            try:\n",
    "                df.to_parquet(output.open(\"wb\"))\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "            return df\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    after = pandas.Timestamp.now() - pandas.to_timedelta(\"7d\")\n",
    "    until = pandas.Timestamp.now() + pandas.to_timedelta(\"1d\")\n",
    "\n",
    "    while after < until:\n",
    "        save_date_issues(after)\n",
    "        after += pandas.to_timedelta(\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7cd85-b3d6-40b9-a505-5228cee0a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_jira_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e09c76-fc69-45c8-a05d-369596e7abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def flatten(nested_dict, parent_key='', sep='_'):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary.\n",
    "    \n",
    "    Args:\n",
    "        nested_dict (dict): The nested dictionary to flatten.\n",
    "        parent_key (str, optional): The parent key for the current level of nesting.\n",
    "        sep (str, optional): The separator to use for flattened keys.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The flattened dictionary.\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    for key, value in nested_dict.items():\n",
    "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            flat_dict.update(flatten(value, new_key, sep))\n",
    "        else:\n",
    "            flat_dict[new_key] = value\n",
    "    \n",
    "    return flat_dict\n",
    "\n",
    "def sentinel_beautify_local(\n",
    "    data: dict,\n",
    "    outputformat: str = \"jira\",\n",
    "    default_status: str = \"Onboard: MOU (T0)\",\n",
    "    default_orgid: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a SecurityIncident including alerts as json and returns\n",
    "    markdown, html and detailed json representation.\n",
    "    \"\"\"\n",
    "    for jsonfield in [\"Labels\", \"Owner\", \"AdditionalData\", \"Comments\"]:\n",
    "        if data.get(jsonfield):\n",
    "            data[jsonfield] = json.loads(data[jsonfield])\n",
    "    labels = [\n",
    "        f\"SIEM_Severity:{data['Severity']}\",\n",
    "        f\"SIEM_Status:{data['Status']}\",\n",
    "        f\"SIEM_Title:{data['Title']}\",\n",
    "    ]\n",
    "    labels += [l[\"labelName\"] for l in data[\"Labels\"]]  # copy over labels from incident\n",
    "    incident_details = [data[\"Description\"], \"\"]\n",
    "\n",
    "    if data.get(\"Owner\"):\n",
    "        owner = None\n",
    "        if data[\"Owner\"].get(\"email\"):\n",
    "            owner = data[\"Owner\"][\"email\"]\n",
    "        elif data[\"Owner\"].get(\"userPrincipalName\"):\n",
    "            owner = data[\"Owner\"][\"userPrincipalName\"]\n",
    "        if owner:\n",
    "            labels.append(f\"SIEM_Owner:{owner}\")\n",
    "            incident_details.append(f\"- **Sentinel Incident Owner:** {owner}\")\n",
    "\n",
    "    if data.get(\"Classification\"):\n",
    "        labels.append(f\"SIEM_Classification:{data['Classification']}\")\n",
    "        incident_details.append(f\"- **Alert Classification:** {data['Classification']}\")\n",
    "\n",
    "    if data.get(\"ClassificationReason\"):\n",
    "        labels.append(f\"SIEM_ClassificationReason:{data['ClassificationReason']}\")\n",
    "        incident_details.append(\n",
    "            f\"- **Alert Classification Reason:** {data['ClassificationReason']}\"\n",
    "        )\n",
    "\n",
    "    if data.get(\"ProviderName\"):\n",
    "        labels.append(f\"SIEM_ProviderName:{data['ProviderName']}\")\n",
    "        incident_details.append(f\"- **Provider Name:** {data['ProviderName']}\")\n",
    "\n",
    "    if data.get(\"AdditionalData\"):\n",
    "        if data[\"AdditionalData\"].get(\"alertProductNames\"):\n",
    "            product_names = \",\".join(data[\"AdditionalData\"][\"alertProductNames\"])\n",
    "            labels.append(f\"SIEM_alertProductNames:{product_names}\")\n",
    "            incident_details.append(f\"- **Product Names:** {product_names}\")\n",
    "        if data[\"AdditionalData\"].get(\"tactics\"):\n",
    "            tactics = \",\".join(data[\"AdditionalData\"][\"tactics\"])\n",
    "            labels.append(f\"SIEM_tactics:{tactics}\")\n",
    "            incident_details.append(\n",
    "                f\"- **[MITRE ATT&CK Tactics](https://attack.mitre.org/tactics/):** {tactics}\"\n",
    "            )\n",
    "        if data[\"AdditionalData\"].get(\"techniques\"):\n",
    "            techniques = \",\".join(data[\"AdditionalData\"][\"techniques\"])\n",
    "            labels.append(f\"SIEM_techniques:{techniques}\")\n",
    "            incident_details.append(\n",
    "                \"- **[MITRE ATT&CK Techniques](https://attack.mitre.org/techniques/):**\"\n",
    "                f\" {techniques}\"\n",
    "            )\n",
    "\n",
    "    comments = []\n",
    "    if data.get(\"Comments\"):\n",
    "        if len(data[\"Comments\"]) > 0:\n",
    "            comments += [\"\", \"## Comments\"]\n",
    "            for comment in data[\"Comments\"]:\n",
    "                comments += comment[\"message\"].split(\"\\n\")\n",
    "            comments += [\"\"]\n",
    "\n",
    "    alert_details = []\n",
    "    observables = []\n",
    "    entity_type_value_mappings = {\n",
    "        \"host\": \"{HostName}\",\n",
    "        \"account\": \"{Name}\",\n",
    "        \"process\": \"{CommandLine}\",\n",
    "        \"file\": \"{Name}\",\n",
    "        \"ip\": \"{Address}\",\n",
    "        \"url\": \"{Url}\",\n",
    "        \"dns\": \"{DomainName}\",\n",
    "        \"registry-key\": \"{Hive}{Key}\",\n",
    "        \"filehash\": \"{Algorithm}{Value}\",\n",
    "    }\n",
    "\n",
    "    class Default(dict):\n",
    "        \"\"\"\n",
    "        Default dict that returns the key if the key is not found\n",
    "        Args:\n",
    "            dict\n",
    "        \"\"\"\n",
    "\n",
    "        def __missing__(self, key):\n",
    "            return key\n",
    "\n",
    "    for alert in data[\"AlertData\"][:10]:  # Assumes alertdata is newest to oldest\n",
    "        if not alert_details:\n",
    "            alert_details += [\n",
    "                \"\",\n",
    "                \"## Alert Details\",\n",
    "                (\n",
    "                    \"The last day of activity (up to 10 alerts) is summarised below from\"\n",
    "                    \" newest to oldest.\"\n",
    "                ),\n",
    "            ]\n",
    "        alert_details.append(\n",
    "            f\"### [{alert['AlertName']} (Severity:{alert['AlertSeverity']}) - \"\n",
    "            + f\"TimeGenerated {alert['TimeGenerated']}]({alert['AlertLink']})\"\n",
    "        )\n",
    "        alert_details.append(alert[\"Description\"])\n",
    "        for key in [\n",
    "            \"RemediationSteps\",\n",
    "            \"ExtendedProperties\",\n",
    "            \"Entities\",\n",
    "        ]:  # entities last as may get truncated\n",
    "            if alert.get(key):\n",
    "                if isinstance(alert[key], str) and alert[key][0] in [\"{\", \"[\"]:\n",
    "                    alert[key] = json.loads(alert[key])\n",
    "                if key == \"Entities\":  # add the entity to our list of observables\n",
    "                    for entity in alert[key]:\n",
    "                        observable = {\"value\": None}\n",
    "                        if \"Type\" in entity:\n",
    "                            observable = {\n",
    "                                \"type\": entity[\"Type\"],\n",
    "                                \"value\": entity_type_value_mappings.get(\n",
    "                                    entity[\"Type\"], \"\"\n",
    "                                ).format_map(Default(entity)),\n",
    "                            }\n",
    "                        if not observable[\"value\"]:  # dump whole dict as string if no mapping found\n",
    "                            observable[\"value\"] = repr(entity)\n",
    "                        observables.append(observable)\n",
    "                if alert[key] and isinstance(alert[key], list) and isinstance(alert[key][0], dict):\n",
    "                    # if list of dicts, make a table\n",
    "                    for index, entry in enumerate(\n",
    "                        [flatten(item) for item in alert[key] if len(item.keys()) > 1]\n",
    "                    ):\n",
    "                        alert_details += [\"\", f\"#### {key}.{index}\"]\n",
    "                        for entrykey, value in entry.items():\n",
    "                            if value:\n",
    "                                alert_details.append(f\"- **{entrykey}:** {value}\")\n",
    "                elif isinstance(alert[key], dict):  # if dict display as list\n",
    "                    alert_details += [\"\", f\"#### {key}\"]\n",
    "                    for entrykey, value in alert[key].items():\n",
    "                        if value and len(value) < 200:\n",
    "                            alert_details.append(f\"- **{entrykey}:** {value}\")\n",
    "                        elif value:  # break out long blocks\n",
    "                            alert_details += [f\"- **{entrykey}:**\", \"\", \"```\", value, \"```\", \"\"]\n",
    "                else:  # otherwise just add as separate lines\n",
    "                    alert_details += [\"\", f\"#### {key}\"] + [item for item in alert[key]]\n",
    "\n",
    "    title = (\n",
    "        f\"SIEM Detection #{data['IncidentNumber']} Sev:{data['Severity']} -\"\n",
    "        f\" {data['Title']} (Status:{data['Status']})\"\n",
    "    )\n",
    "    mdtext = (\n",
    "        [\n",
    "            f\"# {title}\",\n",
    "            \"\",\n",
    "            f\"## [SecurityIncident #{data['IncidentNumber']} Details]({data['IncidentUrl']})\",\n",
    "            \"\",\n",
    "        ]\n",
    "        + incident_details\n",
    "        + comments\n",
    "        + alert_details\n",
    "    )\n",
    "    mdtext = \"\\n\".join([str(line) for line in mdtext])\n",
    "    content = markdown(mdtext, extensions=[\"tables\"])\n",
    "    # remove special chars and deduplicate labels\n",
    "    labels = set(\"\".join(c for c in label if c.isalnum() or c in \".:_\") for label in labels)\n",
    "\n",
    "    response = {\n",
    "        \"subject\": title,\n",
    "        \"labels\": list(labels),\n",
    "        \"observables\": [dict(ts) for ts in set(tuple(i.items()) for i in observables)],\n",
    "        \"sentinel_data\": data,\n",
    "    }\n",
    "    workspaces_df = api.list_workspaces()\n",
    "    customer = (\n",
    "        workspaces_df[workspaces_df[\"customerId\"] == data[\"TenantId\"]].to_dict(\"records\")\n",
    "    )\n",
    "    if len(customer) > 0:\n",
    "        customer = customer[0]\n",
    "    else:\n",
    "        customer = {}\n",
    "    # Grab wiki format for jira and truncate to 32767 chars\n",
    "    response.update(\n",
    "        {\n",
    "            \"secops_status\": customer.get(\"SecOps Status\") or default_status,\n",
    "            \"jira_orgid\": customer.get(\"JiraOrgId\") or default_orgid,\n",
    "            \"customer\": customer,\n",
    "            \"wikimarkup\": (\n",
    "                api.atlaskit_transformer(mdtext)[:32760]\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c1cb2-5d3f-4c33-af56-3a4304d197ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab latest incident with alerts\n",
    "incident = api.security_incidents().dropna(subset=[\"AlertIds\"]).iloc[0]\n",
    "df = api.security_alerts()\n",
    "df = df[df[\"TenantId\"] == incident[\"TenantId\"]]\n",
    "alertids = json.loads(incident[\"AlertIds\"])\n",
    "# extend incident with alert info\n",
    "incident[\"AlertData\"] = df[df[\"SystemAlertId\"].isin(alertids)].copy(deep=True).to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e416d-bbf0-4512-bcc6-bcc3cd02831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a jira friendly format\n",
    "sentinel_beautify_local(incident.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef560e4c-4b15-4d91-b09b-31cb1e12f8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
